#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤ GCS –¥–ª—è GitHub Pages
"""

import json
import os
import re
from datetime import datetime
from typing import List, Dict, Any
from google.cloud import storage
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RANIndexBuilder:
    def __init__(self, bucket_name: str = "journals-ran", output_dir: str = "data"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–ª–¥–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞
        
        Args:
            bucket_name: –ò–º—è GCS bucket
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        """
        self.bucket_name = bucket_name
        self.output_dir = output_dir
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GCS
        try:
            self.storage_client = storage.Client()
            self.bucket = self.storage_client.bucket(bucket_name)
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω –∫ GCS bucket: {bucket_name}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ GCS: {e}")
            self.bucket = None
    
    def collect_all_metadata(self) -> List[Dict[str, Any]]:
        """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ metadata.json —Ñ–∞–π–ª—ã –∏–∑ GCS"""
        if not self.bucket:
            logger.error("–ù–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ GCS")
            return []
        
        articles = []
        processed_count = 0
        error_count = 0
        
        logger.info("üîç –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ GCS...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ metadata.json —Ñ–∞–π–ª—ã
        blobs = self.bucket.list_blobs(prefix="journals/")
        
        for blob in blobs:
            if blob.name.endswith("metadata.json"):
                try:
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata_content = blob.download_as_text()
                    metadata = json.loads(metadata_content)
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    article = self.process_metadata(metadata, blob.name)
                    if article:
                        articles.append(article)
                        processed_count += 1
                        
                        if processed_count % 100 == 0:
                            logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} —Å—Ç–∞—Ç–µ–π...")
                
                except Exception as e:
                    error_count += 1
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {blob.name}: {e}")
        
        logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {processed_count} —Å—Ç–∞—Ç–µ–π, –æ—à–∏–±–æ–∫: {error_count}")
        return articles
    
    def process_metadata(self, metadata: Dict, blob_path: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º metadata.json –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        
        Args:
            metadata: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ metadata.json
            blob_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ GCS
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å: journals/2075-8251/19395/293590/metadata.json
            path_parts = blob_path.split('/')
            if len(path_parts) < 5:
                logger.warning(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—É—Ç–∏: {blob_path}")
                return None
            
            journal_id = path_parts[1]
            issue_id = path_parts[2] 
            article_id = path_parts[3]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            journal_info = metadata.get("journal", {})
            issue_info = metadata.get("issue", {})
            article_info = metadata.get("metadata", {})
            download_links = metadata.get("download_links", {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –≤—ã–ø—É—Å–∫–∞ –∏–ª–∏ issue_info
            year = self.extract_year(issue_info.get("title", ""), issue_info.get("year"))
            
            # –û—á–∏—â–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            authors = self.clean_authors(article_info.get("authors", []))
            keywords = self.clean_keywords(article_info.get("keywords", []))
            abstract = self.clean_text(article_info.get("abstract", ""))
            title = self.clean_text(article_info.get("title", ""))
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç–∞—Ç—å–∏
            article = {
                "id": f"{journal_id}_{issue_id}_{article_id}",
                "journal": {
                    "id": journal_id,
                    "title": journal_info.get("title", "").strip(),
                    "url": journal_info.get("url", "")
                },
                "issue": {
                    "id": issue_id,
                    "title": issue_info.get("title", "").strip(),
                    "year": year,
                    "url": issue_info.get("url", "")
                },
                "article": {
                    "id": article_id,
                    "title": title,
                    "authors": authors,
                    "abstract": abstract,
                    "keywords": keywords,
                    "doi": article_info.get("doi", "").strip(),
                    "pages": article_info.get("pages", "").strip()
                },
                "files": {
                    "pdf_ru": f"gs://{self.bucket_name}/journals/{journal_id}/{issue_id}/{article_id}/article_ru.pdf",
                    "pdf_en": f"gs://{self.bucket_name}/journals/{journal_id}/{issue_id}/{article_id}/article_en.pdf",
                    "xml": f"gs://{self.bucket_name}/journals/{journal_id}/{issue_id}/{article_id}/article.xml"
                },
                "download_links": {
                    "pdf_ru": download_links.get("pdf_ru"),
                    "pdf_en": download_links.get("pdf_en"),
                    "xml": download_links.get("xml")
                },
                "indexed_date": datetime.now().isoformat()
            }
            
            return article
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ {blob_path}: {e}")
            return None
    
    def extract_year(self, issue_title: str, year_field: Any = None) -> int:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ–¥ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –≤—ã–ø—É—Å–∫–∞ –∏–ª–∏ –ø–æ–ª—è –≥–æ–¥–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª–µ year
        if year_field:
            try:
                return int(year_field)
            except:
                pass
        
        # –ò—â–µ–º –≥–æ–¥ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤—ã–ø—É—Å–∫–∞
        if issue_title:
            year_match = re.search(r'20\d{2}', issue_title)
            if year_match:
                return int(year_match.group())
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–µ–∫—É—â–∏–π –≥–æ–¥
        return datetime.now().year
    
    def clean_authors(self, authors: List[str]) -> List[str]:
        """–û—á–∏—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ—Ä–æ–≤ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not authors:
            return []
        
        cleaned = []
        seen = set()
        
        for author in authors:
            if not author:
                continue
                
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            author = author.strip()
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ä–µ–≥–∏—Å—Ç—Ä)
            author_lower = author.lower()
            if author_lower not in seen:
                cleaned.append(author)
                seen.add(author_lower)
        
        return cleaned
    
    def clean_keywords(self, keywords: List[str]) -> List[str]:
        """–û—á–∏—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        if not keywords:
            return []
        
        cleaned = []
        for keyword in keywords:
            if keyword and keyword.strip():
                cleaned.append(keyword.strip())
        
        return list(set(cleaned))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤"""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text.strip())
        
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        text = re.sub(r'<[^>]+>', '', text)
        
        return text
    
    def build_lunr_index(self, articles: List[Dict]) -> Dict:
        """
        –°—Ç—Ä–æ–∏—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è Lunr.js
        
        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
            
        Returns:
            –°–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å Lunr.js
        """
        logger.info("üîß –°—Ç—Ä–æ–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        documents = []
        for i, article in enumerate(articles):
            doc = {
                'id': str(i),
                'title': article['article'].get('title', ''),
                'authors': ' '.join(article['article'].get('authors', [])),
                'abstract': article['article'].get('abstract', ''),
                'keywords': ' '.join(article['article'].get('keywords', [])),
                'journal': article['journal'].get('title', ''),
                'doi': article['article'].get('doi', ''),
                'year': str(article['issue'].get('year', ''))
            }
            documents.append(doc)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è lunr.js
        index_config = {
            "version": "2.3.9",
            "fields": [
                {"fieldName": "title", "boost": 10},
                {"fieldName": "authors", "boost": 5},
                {"fieldName": "keywords", "boost": 8},
                {"fieldName": "abstract", "boost": 2},
                {"fieldName": "journal", "boost": 3},
                {"fieldName": "doi", "boost": 15},
                {"fieldName": "year", "boost": 1}
            ],
            "ref": "id",
            "documents": documents
        }
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return index_config
    
    def generate_website_data(self) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ–±-—Å–∞–π—Ç–∞"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∞–π—Ç–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        os.makedirs(self.output_dir, exist_ok=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏
        articles = self.collect_all_metadata()
        
        if not articles:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–∞—Ç—å—è—Ö")
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = self.generate_stats(articles)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        articles_data = {
            "articles": articles,
            "stats": stats
        }
        
        articles_file = os.path.join(self.output_dir, "articles.json")
        with open(articles_file, 'w', encoding='utf-8') as f:
            json.dump(articles_data, f, ensure_ascii=False, indent=None, separators=(',', ':'))
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–∞—Ç—å—è—Ö: {articles_file}")
        
        # –°—Ç—Ä–æ–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å  
        search_index = self.build_lunr_index(articles)
        
        index_file = os.path.join(self.output_dir, "search-index.json")
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, separators=(',', ':'))
        
        logger.info(f"üîç –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {index_file}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        self.generate_journals_list(articles)
        self.generate_sitemap(articles)
        
        logger.info("üéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   üìö –°—Ç–∞—Ç–µ–π: {stats['total_articles']}")
        logger.info(f"   üèõÔ∏è –ñ—É—Ä–Ω–∞–ª–æ–≤: {stats['total_journals']}")
        logger.info(f"   üìÖ –ì–æ–¥—ã: {stats['year_range'][0]}-{stats['year_range'][1]}")
    
    def generate_stats(self, articles: List[Dict]) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—å—è–º"""
        journals = set()
        years = []
        authors = set()
        keywords = set()
        
        for article in articles:
            journals.add(article['journal']['id'])
            years.append(article['issue']['year'])
            
            for author in article['article']['authors']:
                authors.add(author)
            
            for keyword in article['article']['keywords']:
                keywords.add(keyword)
        
        return {
            "total_articles": len(articles),
            "total_journals": len(journals),
            "total_authors": len(authors),
            "total_keywords": len(keywords),
            "year_range": [min(years) if years else 2020, max(years) if years else 2025],
            "last_updated": datetime.now().isoformat()
        }
    
    def generate_journals_list(self, articles: List[Dict]) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∂—É—Ä–Ω–∞–ª–æ–≤"""
        journals_map = {}
        
        for article in articles:
            journal_id = article['journal']['id']
            if journal_id not in journals_map:
                journals_map[journal_id] = {
                    "id": journal_id,
                    "title": article['journal']['title'],
                    "url": article['journal']['url'],
                    "article_count": 0
                }
            journals_map[journal_id]["article_count"] += 1
        
        journals_list = list(journals_map.values())
        journals_list.sort(key=lambda x: x['title'])
        
        journals_file = os.path.join(self.output_dir, "journals.json")
        with open(journals_file, 'w', encoding='utf-8') as f:
            json.dump({
                "journals": journals_list,
                "total": len(journals_list)
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üìã –°–æ—Ö—Ä–∞–Ω–µ–Ω —Å–ø–∏—Å–æ–∫ –∂—É—Ä–Ω–∞–ª–æ–≤: {journals_file}")
    
    def generate_sitemap(self, articles: List[Dict]) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç sitemap.xml"""
        sitemap_content = ['<?xml version="1.0" encoding="UTF-8"?>']
        sitemap_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
        
        # –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        sitemap_content.append('  <url>')
        sitemap_content.append('    <loc>https://username.github.io/ran-articles-search/</loc>')
        sitemap_content.append('    <changefreq>daily</changefreq>')
        sitemap_content.append('    <priority>1.0</priority>')
        sitemap_content.append('  </url>')
        
        # –°—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç–µ–π (–¥–ª—è –ø–µ—Ä–≤—ã—Ö 1000)
        for article in articles[:1000]:
            sitemap_content.append('  <url>')
            sitemap_content.append(f'    <loc>https://username.github.io/ran-articles-search/#article-{article["id"]}</loc>')
            sitemap_content.append('    <changefreq>monthly</changefreq>')
            sitemap_content.append('    <priority>0.7</priority>')
            sitemap_content.append('  </url>')
        
        sitemap_content.append('</urlset>')
        
        sitemap_file = "sitemap.xml"
        with open(sitemap_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(sitemap_content))
        
        logger.info(f"üó∫Ô∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω sitemap: {sitemap_file}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    bucket_name = os.getenv("GCS_BUCKET_NAME", "journals-ran")
    output_dir = os.getenv("OUTPUT_DIR", "data")
    
    # –°–æ–∑–¥–∞–µ–º –±–∏–ª–¥–µ—Ä –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    builder = RANIndexBuilder(bucket_name, output_dir)
    builder.generate_website_data()

if __name__ == "__main__":
    main()